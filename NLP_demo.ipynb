{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/truongthuanr/transfer/blob/main/NLP_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJWqkM-86ba"
      },
      "source": [
        "# CommonLit Readability problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ful_a-P1fyNg"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICNz7Ij9Xz1"
      },
      "source": [
        "### Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0__CG_5-LIH"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkAT_Fy0N7gh"
      },
      "source": [
        "### Line wrap setting for notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8F0IqUiNdBH"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeY1xaWGOBd-"
      },
      "source": [
        "### Get helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S77IzQYx-PL3"
      },
      "source": [
        "# Get the heper function \n",
        "!wget https://raw.githubusercontent.com/truongthuanr/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import series of helper function for the notebook\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hFEZjXTAC7H"
      },
      "source": [
        "## Get text data set\n",
        "The data set we going to using is from Kagle \"CommonLit Readability Prize\" Competetion\n",
        "https://www.kaggle.com/c/commonlitreadabilityprize/data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB16sm_qAjvd"
      },
      "source": [
        "# Unzip data\n",
        "# !unzip \"/content/drive/MyDrive/01_Personal/01_Study/01_ML/02_CommonLit_Readability/commonlitreadabilityprize.zip\" -d \"/content/drive/MyDrive/01_Personal/01_Study/01_ML/02_CommonLit_Readability/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MTkST59_Qko"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7CLf-7BAyAZ"
      },
      "source": [
        "## Visualizing a text dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqmmvIAxGWvl"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOFOW9TDM1h"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/01_Personal/01_Study/01_ML/02_CommonLit_Readability/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/01_Personal/01_Study/01_ML/02_CommonLit_Readability/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufFIqQcKFEDE"
      },
      "source": [
        "train_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfxFDj3ADr8s"
      },
      "source": [
        "# Shuffle training dataframe\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVTrqM1OEAyt"
      },
      "source": [
        "# train_df_shuffled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1aY_TTJEvxZ"
      },
      "source": [
        "# How the target look like\n",
        "fig, axes = plt.subplots(1,2,figsize=(10, 5))\n",
        "sns.kdeplot(train_df.target,color='navy',shade=True, ax=axes[0]) # Plot the target\n",
        "sns.kdeplot(train_df.standard_error,color='lightseagreen',shade=True, ax=axes[1]) # Plot the sandard_error\n",
        "\n",
        "# Set the title\n",
        "axes[0].set_title('Target Distribution')\n",
        "axes[1].set_title('Standard Error Distribution')\n",
        "\n",
        "axes[0].axvline(train_df.target.mean(),color=\"navy\",linestyle=':')\n",
        "axes[1].axvline(train_df.standard_error.mean(),color=\"navy\",linestyle=':')\n",
        "\n",
        "axes[0].annotate('mean', xy=(-0.3* np.pi, 0.2), xytext=(1, 0.2),\n",
        "            arrowprops=dict(arrowstyle=\"->\",\n",
        "                            connectionstyle=\"angle3,angleA=0,angleB=-90\"));\n",
        "axes[1].annotate('mean', xy=(0.49, 6), xytext=(0.57, 6),\n",
        "            arrowprops=dict(arrowstyle=\"->\",\n",
        "                            connectionstyle=\"angle3,angleA=0,angleB=-90\"));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGPbUV-hqjYy"
      },
      "source": [
        "sns.jointplot(x=train_df.target, \n",
        "              y=train_df.standard_error,\n",
        "              kind='hex',\n",
        "              height=10,\n",
        "              edgecolor='coral',\n",
        "              color='mediumpurple')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8-m_yWWFADW"
      },
      "source": [
        "# How many total sample?\n",
        "len(train_df), len(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WPpaziHKKdH"
      },
      "source": [
        "# visualize some random training examples\n",
        "import random\n",
        "random_index = random.randint(0, len(train_df)-2)\n",
        "for row in train_df_shuffled[[\"excerpt\",\"target\"]][random_index:random_index+2].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f\"Target: {target}\")\n",
        "  print(f\"{text}\\n\")\n",
        "  print(\"---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pIAvjutQAW"
      },
      "source": [
        "# Take a look on other column\n",
        "plt.figure(figsize=(10,5))\n",
        "# Use the count plot with horizontal bar due to long label of license\n",
        "sns.countplot(data=train_df,y='license',palette='viridis')\n",
        "\n",
        "plt.title('License Distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCL7EYykLa9t"
      },
      "source": [
        "## Split data into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTD-jJalOa4p"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt8QN6LQOhJ4"
      },
      "source": [
        "# Use train_test_split to split trainind data into training and validation sets\n",
        "train_excerpt, val_excerpt, train_target, val_target = train_test_split(train_df_shuffled[\"excerpt\"].to_numpy(),\n",
        "                                                                       train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                       test_size=0.1,\n",
        "                                                                       random_state=41)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXFF_R3yPlCX"
      },
      "source": [
        "len(train_excerpt), len(train_target), len(val_excerpt), len(val_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q09TzeS5Pyka"
      },
      "source": [
        "## Converting text in to number\n",
        "When dealing with the text problem, one of the first things we have to do before you can build a model is to convert our text to number.\n",
        "\n",
        "There are a few ways to do this, namely:\n",
        "* Tokenziation - direct mapping of token (a token could be a word or a character) to number\n",
        "* Embedding - create a matrix of feature vector for each token (the size of the feature vector canbe defined and hthis embedding can be learned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySkylMrCQa2y"
      },
      "source": [
        "### Text vectorization (Tokenization)\n",
        "\n",
        "Info: https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgYQUJOJSxIN"
      },
      "source": [
        "# train_excerpt[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "partm3R3S1ac"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Use the default TextVectorization parameters\n",
        "text_vectorizer = TextVectorization(max_tokens=None, # how many word in the vocabulary\n",
        "                                    standardize=\"lower_and_strip_punctuation\",\n",
        "                                    split=\"whitespace\",\n",
        "                                    ngrams=None,\n",
        "                                    output_mode=\"int\", # how to map tokens to number\n",
        "                                    output_sequence_length=None, # how long do we want our sequences to be\n",
        "                                    pad_to_max_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQXOfroNVjHf"
      },
      "source": [
        "# Find the avarage number of tokens (words) in the training excerpt\n",
        "round(sum([len(i.split()) for i in train_excerpt])/len(train_excerpt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hxRXrBIWis2"
      },
      "source": [
        "# Setup the vectorization variables \n",
        "max_vocab_length = 10000 # max number of words to have in our vocabolary\n",
        "max_length = 173 # max length our excerpt will be\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_mode=\"int\",\n",
        "                                    output_sequence_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53AyE0DfCpn_"
      },
      "source": [
        "# Fit the text Vectorizer to the text training data\n",
        "text_vectorizer.adapt(train_excerpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej8Q8ZPhODCR"
      },
      "source": [
        "# Create a sample excerpt\n",
        "sample_excerpt = '''\\\n",
        "The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. \\\n",
        "Early work showed that a linear perceptron cannot be a universal classifier, but that a \\\n",
        "network with a nonpolynomial activation function with one hidden layer of unbounded width can. \\\n",
        "Deep learning is a modern variation which is concerned with an unbounded number of layers of \\\n",
        "bounded size, which permits practical application and optimized implementation, while retaining \\\n",
        "theoretical universality under mild conditions.\n",
        "In deep learning, each level learns to transform its input data into a slightly more abstract and \\\n",
        "composite representation. In an image recognition application, the raw input may be a matrix of pixels; \\\n",
        "the first representational layer may abstract the pixels and encode edges; the second layer may \\\n",
        "compose and encode arrangements of edges; the third layer may encode a nose and eyes; \\\n",
        "and the fourth layer may recognize that the image contains a face. Importantly, a deep learning \\\n",
        "process can learn which features to optimally place in which level on its own.\n",
        "'''\n",
        "\n",
        "text_vectorizer([sample_excerpt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykUWjaYlPVHw"
      },
      "source": [
        "# Choose a random excerpt from the training dataset and tokenize it \n",
        "random_excerpt = random.choice(train_excerpt)\n",
        "print(f\"Origin excerpt:\\n {random_excerpt}\\\n",
        "        \\n\\nVectorize version:\")\n",
        "text_vectorizer([random_excerpt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW_-BilZTMeY"
      },
      "source": [
        "# Get the unique word in the vocabolary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary() # Get all of the unique words\n",
        "top_5_words = words_in_vocab[:5] # get the most common words\n",
        "bottom_5_words = words_in_vocab[-5:]\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"5 most common words: {top_5_words}\" )\n",
        "print(f\"5 least common words: {bottom_5_words}\")\n",
        "\n",
        "top_common_word = words_in_vocab[:20]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsa5LmfQT3ie"
      },
      "source": [
        "### Creating an Embedding using an Embedding Layer\n",
        "\n",
        "We've got a way to map our text to numbers. How about we go a step further and turn those numbers into an embedding?\n",
        "\n",
        "The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. `1` = I, `2` = love, `3` = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.\n",
        "\n",
        "We can see what an embedding of a word looks like by using the [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer. \n",
        "\n",
        "The main parameters we're concerned about here are:\n",
        "* `input_dim` - The size of the vocabulary (e.g. `len(text_vectorizer.get_vocabulary()`).\n",
        "* `output_dim` - The size of the output embedding vector, for example, a value of `100` outputs a  feature vector of size 100 for each word.\n",
        "* `embeddings_initializer` - How to initialize the embeddings matrix, default is `\"uniform\"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n",
        "* `input_length` - Length of sequences being passed to embedding layer.\n",
        "\n",
        "Knowing these, let's make an embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZtnf6Bjh3is"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length) # how long is each input\n",
        "\n",
        "embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N-BeUjLiec3"
      },
      "source": [
        "# Get a random excerpt from the training set \n",
        "random_excerpt = random.choice(train_excerpt)\n",
        "print(f\"Origin excerpt:\\n {random_excerpt}\\\n",
        "        \\n\\nEmbedded version:\")\n",
        "\n",
        "# Embed the random excerpt (turn it into dense vector of fixed size)\n",
        "sample_embed = embedding(text_vectorizer([random_excerpt]))\n",
        "sample_embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuY_zIgRjT0Y"
      },
      "source": [
        "# Check out a single token's embedding\n",
        "sample_embed[0][0], sample_embed[0][0].shape, random_excerpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAawYkwsj0qo"
      },
      "source": [
        "## Modeling a text dataset (running a series of experiments)\n",
        "\n",
        "Now we've a got way to turn out text sequences into numbers, it's time to start building a series of modelling experiments.\n",
        "We'll start with a baseline and move on from there. \n",
        "\n",
        "* **Model 0**: Naive Bayes (baseline)\n",
        "* **Model 1**: Feed-forward neural network (dense model)\n",
        "* **Model 2**: LSTM model\n",
        "* **Model 3**: GRU model\n",
        "* **Model 4**: Bidirectional-LSTM model\n",
        "* **Model 5**: 1D Convolutional Neural Network\n",
        "* **Model 6**: TensorFlow Hub Pretrained Feature Extractor (using transfer learning for NLP)\n",
        "* **Model 7**: Same as model 6 with 10% of training data\n",
        "\n",
        "Use the standard step in modellign with tensorflow:\n",
        "* Create a model\n",
        "* Build a model\n",
        "* Fit a model \n",
        "* Evaluate a model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVSrz4tPmtjj"
      },
      "source": [
        "### Model 0: Geting a baseline\n",
        "As with all machine learning modelling experiments, it's important to create a baselinemodel so you've got a benchmark for future experiments to build upon.\n",
        "\n",
        "To create baseline, use sklearn's multinomial Naive Bayes using the TF-IDF formula to convert our words t onumbers. \n",
        "\n",
        "> **Note:** It's common practice to use non-DL algorithms as a baseline because of their speed and then later using DL to see if you can improve upon them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15fTLdhHDkc3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "            (\"tfidf\", TfidfVectorizer()), # Convert words to numbers using tfidf\n",
        "            (\"r\", Ridge(alpha=0.5)) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_excerpt, train_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMObvl9oFq0n"
      },
      "source": [
        "predict = model_0.predict(val_excerpt)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "score = np.sqrt(mean_squared_error(val_target,predict))\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZJUePUWV6sR"
      },
      "source": [
        "### **Model 1:** A simple Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRexSXkGUhH6"
      },
      "source": [
        "# Create a tensorboard callback (need to craete a new one for each model)\n",
        "\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create a dice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGvNBpUKX6mx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GGBVzYuU_OV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}