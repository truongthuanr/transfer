{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1gaH73m733dAI_-pcnKVtY59iGRmWiw2r",
      "authorship_tag": "ABX9TyNX1pOTJycZEN+2l800CFrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/truongthuanr/transfer/blob/main/NLP_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJWqkM-86ba"
      },
      "source": [
        "# CommonLit Readability problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICNz7Ij9Xz1"
      },
      "source": [
        "### Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0__CG_5-LIH"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkAT_Fy0N7gh"
      },
      "source": [
        "### Line wrap setting for notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8F0IqUiNdBH"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeY1xaWGOBd-"
      },
      "source": [
        "### Get helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S77IzQYx-PL3"
      },
      "source": [
        "# Get the heper function \n",
        "!wget https://raw.githubusercontent.com/truongthuanr/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import series of helper function for the notebook\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hFEZjXTAC7H"
      },
      "source": [
        "## Get text data set\n",
        "The data set we going to using is from Kagle \"CommonLit Readability Prize\" Competetion\n",
        "https://www.kaggle.com/c/commonlitreadabilityprize/data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB16sm_qAjvd"
      },
      "source": [
        "# Unzip data\n",
        "# !unzip \"/content/drive/MyDrive/01_Personal/01_Study/01_ML/02_CommonLit_Readability/commonlitreadabilityprize.zip\" -d \"/content/drive/MyDrive/01_Personal/01_Study/01_ML/02_CommonLit_Readability/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7CLf-7BAyAZ"
      },
      "source": [
        "## Visualizing a text dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqmmvIAxGWvl"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOFOW9TDM1h"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/01_Personal/01_Study/01_ML/02_CommonLit_Readability/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/01_Personal/01_Study/01_ML/02_CommonLit_Readability/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufFIqQcKFEDE"
      },
      "source": [
        "train_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfxFDj3ADr8s"
      },
      "source": [
        "# Shuffle training dataframe\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVTrqM1OEAyt"
      },
      "source": [
        "# train_df_shuffled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1aY_TTJEvxZ"
      },
      "source": [
        "# How the target look like\n",
        "sns.histplot(train_df.target, bins=20,color='navy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8-m_yWWFADW"
      },
      "source": [
        "# How many total sample?\n",
        "len(train_df), len(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WPpaziHKKdH"
      },
      "source": [
        "# visualize some random training examples\n",
        "import random\n",
        "random_index = random.randint(0, len(train_df)-5)\n",
        "for row in train_df_shuffled[[\"excerpt\",\"target\"]][random_index:random_index+5].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f\"Target: {target}\")\n",
        "  print(f\"{text}\\n\")\n",
        "  print(\"---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCL7EYykLa9t"
      },
      "source": [
        "## Split data into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTD-jJalOa4p"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt8QN6LQOhJ4"
      },
      "source": [
        "# Use train_test_split to split trainind data into training and validation sets\n",
        "train_excerpt, val_excerpt, train_taget, val_target = train_test_split(train_df_shuffled[\"excerpt\"].to_numpy(),\n",
        "                                                                       train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                       test_size=0.1,\n",
        "                                                                       random_state=41)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXFF_R3yPlCX"
      },
      "source": [
        "len(train_excerpt), len(train_taget), len(val_excerpt), len(val_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q09TzeS5Pyka"
      },
      "source": [
        "## Converting text in to number\n",
        "When dealing with the text problem, one of the first things we have to do before you can build a model is to convert our text to number.\n",
        "\n",
        "There are a few ways to do this, namely:\n",
        "* Tokenziation - direct mapping of token (a token could be a word or a character) to number\n",
        "* Embedding - create a matrix of feature vector for each token (the size of the feature vector canbe defined and hthis embedding can be learned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySkylMrCQa2y"
      },
      "source": [
        "### Text vectorization (Tokenization)\n",
        "\n",
        "Info: https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgYQUJOJSxIN"
      },
      "source": [
        "# train_excerpt[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "partm3R3S1ac"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Use the default TextVectorization parameters\n",
        "text_vectorizer = TextVectorization(max_tokens=None, # how many word in the vocabulary\n",
        "                                    standardize=\"lower_and_strip_punctuation\",\n",
        "                                    split=\"whitespace\",\n",
        "                                    ngrams=None,\n",
        "                                    output_mode=\"int\", # how to map tokens to number\n",
        "                                    output_sequence_length=None, # how long do we want our sequences to be\n",
        "                                    pad_to_max_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQXOfroNVjHf"
      },
      "source": [
        "# Find the avarage number of tokens (words) in the training excerpt\n",
        "round(sum([len(i.split()) for i in train_excerpt])/len(train_excerpt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hxRXrBIWis2"
      },
      "source": [
        "# Setup the vectorization variables \n",
        "max_vocab_length = 10000 # max number of words to have in our vocabolary\n",
        "max_length = 173 # max length our excerpt will be\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_mode=\"int\",\n",
        "                                    output_sequence_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53AyE0DfCpn_"
      },
      "source": [
        "# Fit the text Vectorizer to the text training data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}